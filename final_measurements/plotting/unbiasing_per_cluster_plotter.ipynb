{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "afe8e782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adler\\OneDrive\\Talent.AI\\TalentAI-research-yasmin\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "\n",
    "os.chdir(r\"C:\\Users\\adler\\OneDrive\\Talent.AI\\TalentAI-research-yasmin\")\n",
    "\n",
    "# Define output directory\n",
    "output_dir = \".\"\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c0034c19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting recommendation algorithm analysis...\n",
      "Processing Excel file: final_measurements/unbiasing/for_company_size/with_gender_and_age_list_frequency_unbiasing_per_cluster.xlsx\n",
      "Successfully read Excel with 508 rows\n",
      "Successfully created DataFrame with 3487 data points\n",
      "Successfully processed data with 3487 data points\n",
      "Processed data saved to 'processed_recommendation_data.csv'\n",
      "Calculating statistics...\n",
      "\n",
      "Overall Statistics:\n",
      "Multiclustering: Records-Rank Correlation = -0.107\n",
      "Standard: Records-Rank Correlation = -0.010\n",
      "Correlation Difference = 0.097\n",
      "Generating records vs rank visualization...\n",
      "Records vs rank visualizations saved\n",
      "Generating rank distribution visualization...\n",
      "Rank distribution visualization saved\n",
      "Generating normalized rank visualization...\n",
      "Normalized rank visualization saved\n",
      "Generating correlation comparison by company...\n",
      "Correlation comparison by company saved\n",
      "Generating analysis report...\n",
      "Analysis report saved as 'recommendation_analysis_report.md'\n",
      "\n",
      "Analysis complete. All visualizations and reports have been saved.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1400x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "def process_excel_data(file_path):\n",
    "    \"\"\"\n",
    "    Process an Excel file containing the recommendation algorithm data\n",
    "    \n",
    "    Parameters:\n",
    "    file_path (str): Path to the Excel file\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: Processed data ready for analysis\n",
    "    \"\"\"\n",
    "    print(f\"Processing Excel file: {file_path}\")\n",
    "    \n",
    "    # Read the Excel file\n",
    "    try:\n",
    "        raw_data = pd.read_excel(file_path)\n",
    "        print(f\"Successfully read Excel with {len(raw_data)} rows\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading Excel file: {e}\")\n",
    "        print(\"Trying to read as TSV file...\")\n",
    "        try:\n",
    "            raw_data = pd.read_csv(file_path, sep='\\t')\n",
    "            print(f\"Successfully read TSV with {len(raw_data)} rows\")\n",
    "        except Exception as e2:\n",
    "            print(f\"Error reading TSV file: {e2}\")\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    # List of all companies that appear in the data\n",
    "    companies = ['Adobe', 'Amazon', 'Apple', 'Facebook', 'Google', 'IBM', \n",
    "                'Microsoft', 'Nvidia', 'Oracle', 'Salesforce', 'Tesla', 'Twitter', 'Uber']\n",
    "    \n",
    "    # Initialize lists to store processed data\n",
    "    all_data_points = []\n",
    "    \n",
    "    # Only process rows up to 184 (your actual data rows)\n",
    "    max_row = min(184, len(raw_data))\n",
    "    \n",
    "    # Process each row\n",
    "    for idx in range(max_row):\n",
    "        row = raw_data.iloc[idx]\n",
    "        try:\n",
    "            # Extract query information - handling it as a string\n",
    "            query_str = str(row.iloc[0])  # Assuming query is in the first column\n",
    "            query_match = re.search(r'\\[(.*?)\\]', query_str)\n",
    "            if not query_match:\n",
    "                # Try alternative: assume the first column is the query\n",
    "                query_info = query_str\n",
    "            else:\n",
    "                query_info = query_match.group(1)\n",
    "            \n",
    "            # Extract dataset and distance function information\n",
    "            dataset_variation = row.iloc[1] if len(row) > 1 else \"\"  # Second column\n",
    "            distance_function = row.iloc[2] if len(row) > 2 else \"\"  # Third column\n",
    "            \n",
    "            # Process Multiclustering data\n",
    "            for i, company in enumerate(companies):\n",
    "                # Calculate column indices for rank and records\n",
    "                rank_idx = 5 + (i * 2)        # Offset by 5 columns, then each company has 2 columns\n",
    "                records_idx = 6 + (i * 2)\n",
    "                \n",
    "                # Safety check\n",
    "                if rank_idx >= len(row) or records_idx >= len(row):\n",
    "                    continue\n",
    "                \n",
    "                # Get rank and records values\n",
    "                rank_val = row.iloc[rank_idx]\n",
    "                records_val = row.iloc[records_idx]\n",
    "                \n",
    "                # Skip if None or NaN\n",
    "                if pd.isna(rank_val) or pd.isna(records_val) or rank_val == 'None' or records_val == 'None':\n",
    "                    continue\n",
    "                    \n",
    "                # Parse values\n",
    "                try:\n",
    "                    if isinstance(rank_val, str) and ',' in rank_val:\n",
    "                        rank = int(rank_val.split(',')[0])\n",
    "                    else:\n",
    "                        rank = int(rank_val)\n",
    "                        \n",
    "                    if isinstance(records_val, str) and ',' in records_val:\n",
    "                        records = int(records_val.split(',')[0])\n",
    "                    else:\n",
    "                        records = int(records_val)\n",
    "                except (ValueError, TypeError):\n",
    "                    continue\n",
    "                    \n",
    "                all_data_points.append({\n",
    "                    'Query': query_info,\n",
    "                    'Dataset': dataset_variation,\n",
    "                    'Distance_Function': distance_function,\n",
    "                    'Algorithm': 'Multiclustering',\n",
    "                    'Company': company,\n",
    "                    'Rank': rank,\n",
    "                    'Records_in_Cluster': records\n",
    "                })\n",
    "            \n",
    "            # Process Standard algorithm data\n",
    "            # Find the 'Standard' column index\n",
    "            standard_idx = -1\n",
    "            for i, val in enumerate(row):\n",
    "                if str(val).strip() == 'Standard':\n",
    "                    standard_idx = i\n",
    "                    break\n",
    "            \n",
    "            if standard_idx == -1:\n",
    "                continue\n",
    "                \n",
    "            for i, company in enumerate(companies):\n",
    "                # Calculate column indices for rank and records\n",
    "                rank_idx = standard_idx + 2 + (i * 2)  # Standard + Nearest Cluster + (company index * 2 columns)\n",
    "                records_idx = standard_idx + 3 + (i * 2)\n",
    "                \n",
    "                # Safety check\n",
    "                if rank_idx >= len(row) or records_idx >= len(row):\n",
    "                    continue\n",
    "                \n",
    "                # Get rank and records values\n",
    "                rank_val = row.iloc[rank_idx]\n",
    "                records_val = row.iloc[records_idx]\n",
    "                \n",
    "                # Skip if None or NaN\n",
    "                if pd.isna(rank_val) or pd.isna(records_val) or rank_val == 'None' or records_val == 'None':\n",
    "                    continue\n",
    "                    \n",
    "                # Parse values\n",
    "                try:\n",
    "                    if isinstance(rank_val, str) and ',' in rank_val:\n",
    "                        rank = int(rank_val.split(',')[0])\n",
    "                    else:\n",
    "                        rank = int(rank_val)\n",
    "                        \n",
    "                    if isinstance(records_val, str) and ',' in records_val:\n",
    "                        records = int(records_val.split(',')[0])\n",
    "                    else:\n",
    "                        records = int(records_val)\n",
    "                except (ValueError, TypeError):\n",
    "                    continue\n",
    "                    \n",
    "                all_data_points.append({\n",
    "                    'Query': query_info,\n",
    "                    'Dataset': dataset_variation,\n",
    "                    'Distance_Function': distance_function,\n",
    "                    'Algorithm': 'Standard',\n",
    "                    'Company': company,\n",
    "                    'Rank': rank,\n",
    "                    'Records_in_Cluster': records\n",
    "                })\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing row {idx}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    result_df = pd.DataFrame(all_data_points)\n",
    "    print(f\"Successfully created DataFrame with {len(result_df)} data points\")\n",
    "    return result_df\n",
    "\n",
    "def visualize_records_vs_rank(df):\n",
    "    \"\"\"\n",
    "    Create plots showing the relationship between Records and Rank for each algorithm\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): Processed data\n",
    "    \"\"\"\n",
    "    print(\"Generating records vs rank visualization...\")\n",
    "    \n",
    "    # Create separate DataFrames for each algorithm\n",
    "    multi_df = df[df['Algorithm'] == 'Multiclustering']\n",
    "    std_df = df[df['Algorithm'] == 'Standard']\n",
    "    \n",
    "    # Calculate correlations\n",
    "    multi_corr = multi_df['Records_in_Cluster'].corr(multi_df['Rank'])\n",
    "    std_corr = std_df['Records_in_Cluster'].corr(std_df['Rank'])\n",
    "    \n",
    "    # Create a scatter plot figure\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    \n",
    "    # Plot for Multiclustering\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.scatter(multi_df['Records_in_Cluster'], multi_df['Rank'], \n",
    "               alpha=0.7, s=60, c='#1f77b4')\n",
    "    \n",
    "    # Add regression line\n",
    "    x = multi_df['Records_in_Cluster']\n",
    "    y = multi_df['Rank']\n",
    "    z = np.polyfit(x, y, 1)\n",
    "    p = np.poly1d(z)\n",
    "    plt.plot(np.sort(x), p(np.sort(x)), \"r--\", linewidth=2)\n",
    "    \n",
    "    # Add annotation with equation\n",
    "    slope, intercept = z\n",
    "    plt.text(0.05, 0.95, f\"y = {slope:.3f}x + {intercept:.3f}\\nr = {multi_corr:.3f}\", \n",
    "             transform=plt.gca().transAxes, fontsize=12, verticalalignment='top',\n",
    "             bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    \n",
    "    plt.title(f'Multiclustering Algorithm', fontsize=16)\n",
    "    plt.xlabel('Number of Records in Cluster', fontsize=14)\n",
    "    plt.ylabel('Rank (Lower is Better)', fontsize=14)\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "    \n",
    "    # Plot for Standard algorithm\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.scatter(std_df['Records_in_Cluster'], std_df['Rank'], \n",
    "               alpha=0.7, s=60, c='#ff7f0e')\n",
    "    \n",
    "    # Add regression line\n",
    "    x = std_df['Records_in_Cluster']\n",
    "    y = std_df['Rank']\n",
    "    z = np.polyfit(x, y, 1)\n",
    "    p = np.poly1d(z)\n",
    "    plt.plot(np.sort(x), p(np.sort(x)), \"r--\", linewidth=2)\n",
    "    \n",
    "    # Add annotation with equation\n",
    "    slope, intercept = z\n",
    "    plt.text(0.05, 0.95, f\"y = {slope:.3f}x + {intercept:.3f}\\nr = {std_corr:.3f}\", \n",
    "             transform=plt.gca().transAxes, fontsize=12, verticalalignment='top',\n",
    "             bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    \n",
    "    plt.title(f'Standard Algorithm', fontsize=16)\n",
    "    plt.xlabel('Number of Records in Cluster', fontsize=14)\n",
    "    plt.ylabel('Rank (Lower is Better)', fontsize=14)\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "    \n",
    "    plt.suptitle('Effect of Cluster Size on Ranking by Algorithm', fontsize=20)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, 'records_vs_rank_comparison.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # Create hexbin plots for data density visualization\n",
    "    plt.figure(figsize=(16, 6))\n",
    "    \n",
    "    # Multiclustering hexbin plot\n",
    "    plt.subplot(1, 2, 1)\n",
    "    hb1 = plt.hexbin(multi_df['Records_in_Cluster'], multi_df['Rank'], \n",
    "                    gridsize=15, cmap='Blues', mincnt=1)\n",
    "    \n",
    "    # Add regression line\n",
    "    x = multi_df['Records_in_Cluster']\n",
    "    y = multi_df['Rank']\n",
    "    z = np.polyfit(x, y, 1)\n",
    "    p = np.poly1d(z)\n",
    "    plt.plot(np.sort(x), p(np.sort(x)), \"r--\", linewidth=2)\n",
    "    \n",
    "    plt.title(f'Multiclustering Algorithm (r={multi_corr:.3f})', fontsize=16)\n",
    "    plt.xlabel('Records in Cluster', fontsize=14)\n",
    "    plt.ylabel('Rank (Lower is Better)', fontsize=14)\n",
    "    plt.colorbar(hb1, label='Count')\n",
    "    plt.grid(alpha=0.3)\n",
    "    \n",
    "    # Standard algorithm hexbin plot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    hb2 = plt.hexbin(std_df['Records_in_Cluster'], std_df['Rank'], \n",
    "                    gridsize=15, cmap='Oranges', mincnt=1)\n",
    "    \n",
    "    # Add regression line\n",
    "    x = std_df['Records_in_Cluster']\n",
    "    y = std_df['Rank']\n",
    "    z = np.polyfit(x, y, 1)\n",
    "    p = np.poly1d(z)\n",
    "    plt.plot(np.sort(x), p(np.sort(x)), \"r--\", linewidth=2)\n",
    "    \n",
    "    plt.title(f'Standard Algorithm (r={std_corr:.3f})', fontsize=16)\n",
    "    plt.xlabel('Records in Cluster', fontsize=14)\n",
    "    plt.ylabel('Rank (Lower is Better)', fontsize=14)\n",
    "    plt.colorbar(hb2, label='Count')\n",
    "    plt.grid(alpha=0.3)\n",
    "    \n",
    "    plt.suptitle('Density of Records vs Rank by Algorithm', fontsize=20)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('density_plot_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    print(\"Records vs rank visualizations saved\")\n",
    "\n",
    "def visualize_rank_distribution(df):\n",
    "    \"\"\"\n",
    "    Create a visualization of rank distributions by algorithm\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): Processed data\n",
    "    \"\"\"\n",
    "    print(\"Generating rank distribution visualization...\")\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Create violin plots for better distribution visualization\n",
    "    sns.violinplot(data=df, x='Algorithm', y='Rank', hue='Algorithm', \n",
    "                  palette=['#1f77b4', '#ff7f0e'], inner='box', legend=False)\n",
    "    \n",
    "    # Add boxplot inside\n",
    "    sns.boxplot(data=df, x='Algorithm', y='Rank', width=0.3, color='white', \n",
    "               boxprops=dict(alpha=.5), showfliers=False)\n",
    "    \n",
    "    # Add strip plot for individual data points\n",
    "    sns.stripplot(data=df, x='Algorithm', y='Rank', size=3, color='black', alpha=0.3, jitter=True)\n",
    "    \n",
    "    # Calculate and display stats\n",
    "    stats = df.groupby('Algorithm')['Rank'].agg(['median', 'mean', 'count'])\n",
    "    \n",
    "    # Add text annotations for statistics\n",
    "    y_max = df['Rank'].max()\n",
    "    for i, alg in enumerate(['Multiclustering', 'Standard']):\n",
    "        if alg in stats.index:\n",
    "            plt.text(i, y_max * 0.9, \n",
    "                    f\"Median: {stats.loc[alg, 'median']:.1f}\\nMean: {stats.loc[alg, 'mean']:.1f}\\nN: {stats.loc[alg, 'count']}\", \n",
    "                    ha='center', va='top', fontsize=11,\n",
    "                    bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    \n",
    "    plt.title('Rank Distribution by Algorithm', fontsize=16)\n",
    "    plt.xlabel('Algorithm', fontsize=14)\n",
    "    plt.ylabel('Rank (Lower is Better)', fontsize=14)\n",
    "    plt.grid(True, linestyle='--', alpha=0.4, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('rank_distribution.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    print(\"Rank distribution visualization saved\")\n",
    "\n",
    "def create_normalized_rank_plot(df):\n",
    "    \"\"\"\n",
    "    Create a plot with normalized ranks to better show the bias effect\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): Processed data\n",
    "    \"\"\"\n",
    "    print(\"Generating normalized rank visualization...\")\n",
    "    \n",
    "    # Create a copy of the dataframe\n",
    "    df_norm = df.copy()\n",
    "    \n",
    "    # Calculate normalized rank\n",
    "    # Formula: normalized_rank = rank / (1 + log(1 + records))\n",
    "    # Lower is better - less influenced by the number of records\n",
    "    df_norm['Normalized_Rank'] = df_norm['Rank'] / (1 + np.log1p(df_norm['Records_in_Cluster']))\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Create violin plots for better distribution visualization\n",
    "    sns.violinplot(data=df_norm, x='Algorithm', y='Normalized_Rank', hue='Algorithm',\n",
    "                  palette=['#1f77b4', '#ff7f0e'], inner='box', legend=False)\n",
    "    \n",
    "    # Add boxplot inside\n",
    "    sns.boxplot(data=df_norm, x='Algorithm', y='Normalized_Rank', width=0.3, color='white', \n",
    "               boxprops=dict(alpha=.5), showfliers=False)\n",
    "    \n",
    "    # Add strip plot for individual data points\n",
    "    sns.stripplot(data=df_norm, x='Algorithm', y='Normalized_Rank', size=3, color='black', alpha=0.3, jitter=True)\n",
    "    \n",
    "    # Calculate and display stats\n",
    "    stats = df_norm.groupby('Algorithm')['Normalized_Rank'].agg(['median', 'mean', 'count'])\n",
    "    \n",
    "    # Add text annotations for statistics\n",
    "    y_max = df_norm['Normalized_Rank'].max()\n",
    "    for i, alg in enumerate(['Multiclustering', 'Standard']):\n",
    "        if alg in stats.index:\n",
    "            plt.text(i, y_max * 0.9, \n",
    "                    f\"Median: {stats.loc[alg, 'median']:.2f}\\nMean: {stats.loc[alg, 'mean']:.2f}\", \n",
    "                    ha='center', va='top', fontsize=11,\n",
    "                    bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    \n",
    "    plt.title('Normalized Rank by Algorithm (Adjusted for Cluster Size)', fontsize=16)\n",
    "    plt.xlabel('Algorithm', fontsize=14)\n",
    "    plt.ylabel('Normalized Rank (Lower is Better)', fontsize=14)\n",
    "    plt.grid(True, linestyle='--', alpha=0.4, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('normalized_rank.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    print(\"Normalized rank visualization saved\")\n",
    "\n",
    "def compare_correlations_by_company(df):\n",
    "    \"\"\"\n",
    "    Create a bar chart comparing correlation coefficients between records and rank\n",
    "    for each company across both algorithms\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): Processed data\n",
    "    \"\"\"\n",
    "    print(\"Generating correlation comparison by company...\")\n",
    "    \n",
    "    companies = df['Company'].unique()\n",
    "    correlations = []\n",
    "    \n",
    "    for company in companies:\n",
    "        company_data = df[df['Company'] == company]\n",
    "        multi_data = company_data[company_data['Algorithm'] == 'Multiclustering']\n",
    "        std_data = company_data[company_data['Algorithm'] == 'Standard']\n",
    "        \n",
    "        # Calculate correlations if enough data\n",
    "        multi_corr = multi_data['Records_in_Cluster'].corr(multi_data['Rank']) if len(multi_data) > 2 else np.nan\n",
    "        std_corr = std_data['Records_in_Cluster'].corr(std_data['Rank']) if len(std_data) > 2 else np.nan\n",
    "        \n",
    "        correlations.append({\n",
    "            'Company': company,\n",
    "            'Multiclustering': multi_corr,\n",
    "            'Standard': std_corr\n",
    "        })\n",
    "    \n",
    "    # Create DataFrame for plotting\n",
    "    corr_df = pd.DataFrame(correlations)\n",
    "    \n",
    "    # Reshape for seaborn\n",
    "    corr_melted = pd.melt(corr_df, id_vars=['Company'], \n",
    "                         value_vars=['Multiclustering', 'Standard'],\n",
    "                         var_name='Algorithm', value_name='Correlation')\n",
    "    \n",
    "    # Create the plot\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    # Sort by Standard algorithm correlation\n",
    "    sorted_companies = corr_df.sort_values(by='Standard', ascending=False)['Company'].tolist()\n",
    "    \n",
    "    # Create a grouped bar chart\n",
    "    g = sns.catplot(data=corr_melted, x='Company', y='Correlation', hue='Algorithm',\n",
    "                   kind='bar', palette=['#1f77b4', '#ff7f0e'], height=6, aspect=2)\n",
    "    \n",
    "    # Adjust the plot\n",
    "    g.set_xticklabels(rotation=45, ha='right')\n",
    "    g.fig.suptitle('Correlation Between Records and Rank by Company', fontsize=16)\n",
    "    g.set(xlabel='Company', ylabel='Correlation Coefficient')\n",
    "    \n",
    "    # Add a horizontal line at zero\n",
    "    plt.axhline(y=0, color='gray', linestyle='-', alpha=0.5)\n",
    "    \n",
    "    # Add overall correlations as text\n",
    "    multi_overall = df[df['Algorithm'] == 'Multiclustering']['Records_in_Cluster'].corr(\n",
    "        df[df['Algorithm'] == 'Multiclustering']['Rank'])\n",
    "    std_overall = df[df['Algorithm'] == 'Standard']['Records_in_Cluster'].corr(\n",
    "        df[df['Algorithm'] == 'Standard']['Rank'])\n",
    "    \n",
    "    plt.text(0.02, 0.95, f\"Overall Correlations:\\nMulticlustering: {multi_overall:.3f}\\nStandard: {std_overall:.3f}\", \n",
    "             transform=plt.gca().transAxes, fontsize=12, verticalalignment='top',\n",
    "             bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('correlation_by_company.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    print(\"Correlation comparison by company saved\")\n",
    "\n",
    "def calculate_statistics(df):\n",
    "    \"\"\"\n",
    "    Calculate and print important statistics from the data\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): Processed data\n",
    "    \n",
    "    Returns:\n",
    "    dict: Summary statistics\n",
    "    \"\"\"\n",
    "    print(\"Calculating statistics...\")\n",
    "    \n",
    "    # Split by algorithm\n",
    "    multi_df = df[df['Algorithm'] == 'Multiclustering']\n",
    "    std_df = df[df['Algorithm'] == 'Standard']\n",
    "    \n",
    "    # Calculate overall statistics\n",
    "    stats = {\n",
    "        'overall': {\n",
    "            'Multiclustering': {\n",
    "                'count': len(multi_df),\n",
    "                'avg_records': multi_df['Records_in_Cluster'].mean(),\n",
    "                'avg_rank': multi_df['Rank'].mean(),\n",
    "                'median_rank': multi_df['Rank'].median(),\n",
    "                'correlation': multi_df['Records_in_Cluster'].corr(multi_df['Rank'])\n",
    "            },\n",
    "            'Standard': {\n",
    "                'count': len(std_df),\n",
    "                'avg_records': std_df['Records_in_Cluster'].mean(),\n",
    "                'avg_rank': std_df['Rank'].mean(),\n",
    "                'median_rank': std_df['Rank'].median(),\n",
    "                'correlation': std_df['Records_in_Cluster'].corr(std_df['Rank'])\n",
    "            }\n",
    "        },\n",
    "        'by_company': {}\n",
    "    }\n",
    "    \n",
    "    # Calculate statistics by company\n",
    "    for company in df['Company'].unique():\n",
    "        company_multi = multi_df[multi_df['Company'] == company]\n",
    "        company_std = std_df[std_df['Company'] == company]\n",
    "        \n",
    "        multi_corr = company_multi['Records_in_Cluster'].corr(company_multi['Rank']) if len(company_multi) > 2 else np.nan\n",
    "        std_corr = company_std['Records_in_Cluster'].corr(company_std['Rank']) if len(company_std) > 2 else np.nan\n",
    "        \n",
    "        stats['by_company'][company] = {\n",
    "            'Multiclustering': {\n",
    "                'count': len(company_multi),\n",
    "                'avg_records': company_multi['Records_in_Cluster'].mean() if len(company_multi) > 0 else np.nan,\n",
    "                'avg_rank': company_multi['Rank'].mean() if len(company_multi) > 0 else np.nan,\n",
    "                'correlation': multi_corr\n",
    "            },\n",
    "            'Standard': {\n",
    "                'count': len(company_std),\n",
    "                'avg_records': company_std['Records_in_Cluster'].mean() if len(company_std) > 0 else np.nan,\n",
    "                'avg_rank': company_std['Rank'].mean() if len(company_std) > 0 else np.nan,\n",
    "                'correlation': std_corr\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    # Print overall statistics\n",
    "    print(\"\\nOverall Statistics:\")\n",
    "    print(f\"Multiclustering: Records-Rank Correlation = {stats['overall']['Multiclustering']['correlation']:.3f}\")\n",
    "    print(f\"Standard: Records-Rank Correlation = {stats['overall']['Standard']['correlation']:.3f}\")\n",
    "    print(f\"Correlation Difference = {stats['overall']['Standard']['correlation'] - stats['overall']['Multiclustering']['correlation']:.3f}\")\n",
    "    \n",
    "    # Create and save overall DataFrame\n",
    "    overall_df = pd.DataFrame({\n",
    "        'Algorithm': ['Multiclustering', 'Standard'],\n",
    "        'Count': [stats['overall']['Multiclustering']['count'], stats['overall']['Standard']['count']],\n",
    "        'Average Records': [stats['overall']['Multiclustering']['avg_records'], stats['overall']['Standard']['avg_records']],\n",
    "        'Average Rank': [stats['overall']['Multiclustering']['avg_rank'], stats['overall']['Standard']['avg_rank']],\n",
    "        'Median Rank': [stats['overall']['Multiclustering']['median_rank'], stats['overall']['Standard']['median_rank']],\n",
    "        'Correlation (Records vs Rank)': [stats['overall']['Multiclustering']['correlation'], stats['overall']['Standard']['correlation']]\n",
    "    })\n",
    "    \n",
    "    overall_df.to_csv('overall_statistics.csv', index=False)\n",
    "    \n",
    "    # Create and save company statistics DataFrame\n",
    "    company_rows = []\n",
    "    for company, company_stats in stats['by_company'].items():\n",
    "        company_rows.append({\n",
    "            'Company': company,\n",
    "            'Multiclustering Count': company_stats['Multiclustering']['count'],\n",
    "            'Standard Count': company_stats['Standard']['count'],\n",
    "            'Multiclustering Avg Records': company_stats['Multiclustering']['avg_records'],\n",
    "            'Standard Avg Records': company_stats['Standard']['avg_records'],\n",
    "            'Multiclustering Avg Rank': company_stats['Multiclustering']['avg_rank'],\n",
    "            'Standard Avg Rank': company_stats['Standard']['avg_rank'],\n",
    "            'Multiclustering Correlation': company_stats['Multiclustering']['correlation'],\n",
    "            'Standard Correlation': company_stats['Standard']['correlation'],\n",
    "            'Correlation Difference': company_stats['Standard']['correlation'] - company_stats['Multiclustering']['correlation']\n",
    "        })\n",
    "    \n",
    "    company_df = pd.DataFrame(company_rows)\n",
    "    company_df.to_csv('company_statistics.csv', index=False)\n",
    "    \n",
    "    return stats\n",
    "\n",
    "def create_analysis_report(df, stats):\n",
    "    \"\"\"\n",
    "    Create a comprehensive analysis report in markdown format\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): Processed data\n",
    "    stats (dict): Statistics dictionary from calculate_statistics\n",
    "    \"\"\"\n",
    "    print(\"Generating analysis report...\")\n",
    "    \n",
    "    report_text = \"\"\"# Recommendation Algorithms Analysis Report\n",
    "\n",
    "## Overview\n",
    "\n",
    "This analysis compares two recommendation algorithms (Multiclustering and Standard) to evaluate how the number of records in a cluster affects ranking performance. The hypothesis being tested is that the Multiclustering algorithm is less affected by cluster size bias than the Standard algorithm.\n",
    "\n",
    "## Key Findings\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "    multi_corr = stats['overall']['Multiclustering']['correlation']\n",
    "    std_corr = stats['overall']['Standard']['correlation']\n",
    "    corr_diff = std_corr - multi_corr\n",
    "    \n",
    "    report_text += f\"- **Correlation between Records and Rank**:\\n\"\n",
    "    report_text += f\"  - Multiclustering: {multi_corr:.3f}\\n\"\n",
    "    report_text += f\"  - Standard: {std_corr:.3f}\\n\"\n",
    "    report_text += f\"  - Difference: {corr_diff:.3f}\\n\\n\"\n",
    "    \n",
    "    if std_corr > multi_corr:\n",
    "        report_text += \"This confirms the hypothesis that the Standard algorithm shows a stronger correlation between the number of records in a cluster and the ranking, indicating it is more biased by cluster size.\\n\\n\"\n",
    "    else:\n",
    "        report_text += \"The data does not support the hypothesis that the Standard algorithm is more biased by cluster size than the Multiclustering algorithm.\\n\\n\"\n",
    "    \n",
    "    report_text += \"## Statistical Summary\\n\\n\"\n",
    "    report_text += \"| Metric | Multiclustering | Standard |\\n\"\n",
    "    report_text += \"|--------|----------------|----------|\\n\"\n",
    "    report_text += f\"| Sample Size | {stats['overall']['Multiclustering']['count']} | {stats['overall']['Standard']['count']} |\\n\"\n",
    "    report_text += f\"| Average Records in Cluster | {stats['overall']['Multiclustering']['avg_records']:.2f} | {stats['overall']['Standard']['avg_records']:.2f} |\\n\"\n",
    "    report_text += f\"| Average Rank | {stats['overall']['Multiclustering']['avg_rank']:.2f} | {stats['overall']['Standard']['avg_rank']:.2f} |\\n\"\n",
    "    report_text += f\"| Median Rank | {stats['overall']['Multiclustering']['median_rank']:.1f} | {stats['overall']['Standard']['median_rank']:.1f} |\\n\"\n",
    "    report_text += f\"| Records-Rank Correlation | {stats['overall']['Multiclustering']['correlation']:.3f} | {stats['overall']['Standard']['correlation']:.3f} |\\n\\n\"\n",
    "    \n",
    "    report_text += \"## Visual Analysis\\n\\n\"\n",
    "    report_text += \"The visualizations demonstrate the relationship between the number of records in a cluster and the ranking performance for each algorithm.\\n\\n\"\n",
    "    report_text += \"### Records vs Rank Plots\\n\\n\"\n",
    "    report_text += \"The scatter plots and density plots show how the number of records in a cluster affects ranking. The regression lines illustrate the trend, and the correlation coefficients quantify the strength of the relationship.\\n\\n\"\n",
    "    report_text += \"### Rank Distribution\\n\\n\"\n",
    "    report_text += \"The rank distribution plots show how rankings are distributed for each algorithm, with lower values indicating better performance.\\n\\n\"\n",
    "    report_text += \"### Normalized Rank\\n\\n\"\n",
    "    report_text += \"The normalized rank visualizations adjust the rank values to account for cluster size, providing a fairer comparison between the algorithms.\\n\\n\"\n",
    "    \n",
    "    report_text += \"## Company-Specific Analysis\\n\\n\"\n",
    "    report_text += \"Different companies show varying levels of bias in the recommendation algorithms:\\n\\n\"\n",
    "    \n",
    "    # Add company-specific insights\n",
    "    companies_higher_std_corr = []\n",
    "    companies_lower_std_corr = []\n",
    "    \n",
    "    for company, stats_dict in stats['by_company'].items():\n",
    "        multi_corr = stats_dict['Multiclustering']['correlation']\n",
    "        std_corr = stats_dict['Standard']['correlation']\n",
    "        \n",
    "        if not np.isnan(multi_corr) and not np.isnan(std_corr):\n",
    "            if std_corr > multi_corr:\n",
    "                companies_higher_std_corr.append(company)\n",
    "            else:\n",
    "                companies_lower_std_corr.append(company)\n",
    "    \n",
    "    report_text += f\"- **Companies where Standard algorithm shows higher bias**: {', '.join(companies_higher_std_corr)}\\n\"\n",
    "    report_text += f\"- **Companies where Multiclustering shows higher bias or equal**: {', '.join(companies_lower_std_corr)}\\n\\n\"\n",
    "    \n",
    "    report_text += \"## Conclusion\\n\\n\"\n",
    "    \n",
    "    if std_corr > multi_corr:\n",
    "        report_text += f\"The analysis confirms the hypothesis that the Standard algorithm is more influenced by the number of records in a cluster (correlation: {std_corr:.3f}) compared to the Multiclustering algorithm (correlation: {multi_corr:.3f}). This indicates that the Multiclustering approach successfully reduces bias related to cluster size in the recommendation system.\\n\\n\"\n",
    "    \n",
    "    # Save the report\n",
    "    with open('recommendation_analysis_report.md', 'w') as f:\n",
    "        f.write(report_text)\n",
    "    \n",
    "    print(\"Analysis report saved as 'recommendation_analysis_report.md'\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to run the analysis\n",
    "    \"\"\"\n",
    "    print(\"Starting recommendation algorithm analysis...\")\n",
    "    \n",
    "    # Process the data file - update this to your actual file path\n",
    "    file_path = \"final_measurements/unbiasing/for_company_size/with_gender_and_age_list_frequency_unbiasing_per_cluster.xlsx\"\n",
    "    # Uncomment the line below to use your specific path\n",
    "    # file_path = \"with_gender_and_age_list_frequency_unbiasing_per_cluster.xlsx\"\n",
    "    \n",
    "    # Process the data\n",
    "    df = process_excel_data(file_path)\n",
    "    \n",
    "    if len(df) == 0:\n",
    "        print(\"No valid data was processed. Please check your data file format.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Successfully processed data with {len(df)} data points\")\n",
    "    \n",
    "    # Save the processed data to CSV for further analysis if needed\n",
    "    df.to_csv('processed_recommendation_data.csv', index=False)\n",
    "    print(\"Processed data saved to 'processed_recommendation_data.csv'\")\n",
    "    \n",
    "    # Calculate statistics\n",
    "    stats = calculate_statistics(df)\n",
    "    \n",
    "    # Create visualizations\n",
    "    visualize_records_vs_rank(df)\n",
    "    visualize_rank_distribution(df)\n",
    "    create_normalized_rank_plot(df)\n",
    "    compare_correlations_by_company(df)\n",
    "    \n",
    "    # Generate final report\n",
    "    create_analysis_report(df, stats)\n",
    "    \n",
    "    print(\"\\nAnalysis complete. All visualizations and reports have been saved.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3364afc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def improved_scatter_comparison(df):\n",
    "    \"\"\"\n",
    "    Create an improved scatter plot comparing both algorithms on the same plot\n",
    "    with clearer regression lines and confidence intervals\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): Processed data\n",
    "    \"\"\"\n",
    "    # Create figure\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Filter data for each algorithm\n",
    "    multi_df = df[df['Algorithm'] == 'Multiclustering']\n",
    "    std_df = df[df['Algorithm'] == 'Standard']\n",
    "    \n",
    "    # Calculate correlations\n",
    "    multi_corr = multi_df['Records_in_Cluster'].corr(multi_df['Rank'])\n",
    "    std_corr = std_df['Records_in_Cluster'].corr(std_df['Rank'])\n",
    "    \n",
    "    # Create scatter plots with alpha for better visibility of overlapping points\n",
    "    plt.scatter(multi_df['Records_in_Cluster'], multi_df['Rank'], \n",
    "               alpha=0.6, s=80, color='#1f77b4', label=f'Multiclustering (r={multi_corr:.3f})')\n",
    "    plt.scatter(std_df['Records_in_Cluster'], std_df['Rank'], \n",
    "               alpha=0.6, s=80, color='#ff7f0e', label=f'Standard (r={std_corr:.3f})')\n",
    "    \n",
    "    # Add regression lines with confidence intervals\n",
    "    from scipy import stats\n",
    "    \n",
    "    # Multiclustering regression with confidence interval\n",
    "    slope, intercept, r_value, p_value, std_err = stats.linregress(multi_df['Records_in_Cluster'], multi_df['Rank'])\n",
    "    x_multi = np.linspace(multi_df['Records_in_Cluster'].min(), multi_df['Records_in_Cluster'].max(), 100)\n",
    "    y_multi = slope * x_multi + intercept\n",
    "    \n",
    "    # Plot regression line\n",
    "    plt.plot(x_multi, y_multi, color='#1f77b4', linestyle='-', linewidth=3)\n",
    "    \n",
    "    # Standard regression with confidence interval\n",
    "    slope, intercept, r_value, p_value, std_err = stats.linregress(std_df['Records_in_Cluster'], std_df['Rank'])\n",
    "    x_std = np.linspace(std_df['Records_in_Cluster'].min(), std_df['Records_in_Cluster'].max(), 100)\n",
    "    y_std = slope * x_std + intercept\n",
    "    \n",
    "    # Plot regression line\n",
    "    plt.plot(x_std, y_std, color='#ff7f0e', linestyle='-', linewidth=3)\n",
    "    \n",
    "    # Add shaded confidence interval (requires additional computation)\n",
    "    \n",
    "    # Add annotations\n",
    "    plt.title('Effect of Cluster Size on Ranking: Algorithm Comparison', fontsize=18, fontweight='bold')\n",
    "    plt.xlabel('Number of Records in Cluster', fontsize=14)\n",
    "    plt.ylabel('Rank (Lower is Better)', fontsize=14)\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Add legend\n",
    "    plt.legend(fontsize=12, loc='upper left')\n",
    "    \n",
    "    # Add correlation difference annotation\n",
    "    correlation_diff = std_corr - multi_corr\n",
    "    plt.annotate(f'Correlation Difference: {correlation_diff:.3f}',\n",
    "                xy=(0.02, 0.95), xycoords='axes fraction',\n",
    "                fontsize=12, bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('improved_algorithm_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2c975105",
   "metadata": {},
   "outputs": [],
   "source": [
    "def improved_scatter_comparison(df):\n",
    "    \"\"\"\n",
    "    Create an improved scatter plot comparing both algorithms on the same plot\n",
    "    with clearer regression lines and confidence intervals\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): Processed data\n",
    "    \"\"\"\n",
    "    # Create figure\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Filter data for each algorithm\n",
    "    multi_df = df[df['Algorithm'] == 'Multiclustering']\n",
    "    std_df = df[df['Algorithm'] == 'Standard']\n",
    "    \n",
    "    # Calculate correlations\n",
    "    multi_corr = multi_df['Records_in_Cluster'].corr(multi_df['Rank'])\n",
    "    std_corr = std_df['Records_in_Cluster'].corr(std_df['Rank'])\n",
    "    \n",
    "    # Create scatter plots with alpha for better visibility of overlapping points\n",
    "    plt.scatter(multi_df['Records_in_Cluster'], multi_df['Rank'], \n",
    "               alpha=0.6, s=80, color='#1f77b4', label=f'Multiclustering (r={multi_corr:.3f})')\n",
    "    plt.scatter(std_df['Records_in_Cluster'], std_df['Rank'], \n",
    "               alpha=0.6, s=80, color='#ff7f0e', label=f'Standard (r={std_corr:.3f})')\n",
    "    \n",
    "    # Add regression lines with confidence intervals\n",
    "    from scipy import stats\n",
    "    \n",
    "    # Multiclustering regression with confidence interval\n",
    "    slope, intercept, r_value, p_value, std_err = stats.linregress(multi_df['Records_in_Cluster'], multi_df['Rank'])\n",
    "    x_multi = np.linspace(multi_df['Records_in_Cluster'].min(), multi_df['Records_in_Cluster'].max(), 100)\n",
    "    y_multi = slope * x_multi + intercept\n",
    "    \n",
    "    # Plot regression line\n",
    "    plt.plot(x_multi, y_multi, color='#1f77b4', linestyle='-', linewidth=3)\n",
    "    \n",
    "    # Standard regression with confidence interval\n",
    "    slope, intercept, r_value, p_value, std_err = stats.linregress(std_df['Records_in_Cluster'], std_df['Rank'])\n",
    "    x_std = np.linspace(std_df['Records_in_Cluster'].min(), std_df['Records_in_Cluster'].max(), 100)\n",
    "    y_std = slope * x_std + intercept\n",
    "    \n",
    "    # Plot regression line\n",
    "    plt.plot(x_std, y_std, color='#ff7f0e', linestyle='-', linewidth=3)\n",
    "    \n",
    "    # Add shaded confidence interval (requires additional computation)\n",
    "    \n",
    "    # Add annotations\n",
    "    plt.title('Effect of Cluster Size on Ranking: Algorithm Comparison', fontsize=18, fontweight='bold')\n",
    "    plt.xlabel('Number of Records in Cluster', fontsize=14)\n",
    "    plt.ylabel('Rank (Lower is Better)', fontsize=14)\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Add legend\n",
    "    plt.legend(fontsize=12, loc='upper left')\n",
    "    \n",
    "    # Add correlation difference annotation\n",
    "    correlation_diff = std_corr - multi_corr\n",
    "    plt.annotate(f'Correlation Difference: {correlation_diff:.3f}',\n",
    "                xy=(0.02, 0.95), xycoords='axes fraction',\n",
    "                fontsize=12, bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('improved_algorithm_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "be8fed0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def improved_scatter_comparison(df):\n",
    "    \"\"\"\n",
    "    Create an improved scatter plot comparing both algorithms on the same plot\n",
    "    with clearer regression lines and confidence intervals\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): Processed data\n",
    "    \"\"\"\n",
    "    # Create figure\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Filter data for each algorithm\n",
    "    multi_df = df[df['Algorithm'] == 'Multiclustering']\n",
    "    std_df = df[df['Algorithm'] == 'Standard']\n",
    "    \n",
    "    # Calculate correlations\n",
    "    multi_corr = multi_df['Records_in_Cluster'].corr(multi_df['Rank'])\n",
    "    std_corr = std_df['Records_in_Cluster'].corr(std_df['Rank'])\n",
    "    \n",
    "    # Create scatter plots with alpha for better visibility of overlapping points\n",
    "    plt.scatter(multi_df['Records_in_Cluster'], multi_df['Rank'], \n",
    "               alpha=0.6, s=80, color='#1f77b4', label=f'Multiclustering (r={multi_corr:.3f})')\n",
    "    plt.scatter(std_df['Records_in_Cluster'], std_df['Rank'], \n",
    "               alpha=0.6, s=80, color='#ff7f0e', label=f'Standard (r={std_corr:.3f})')\n",
    "    \n",
    "    # Add regression lines with confidence intervals\n",
    "    from scipy import stats\n",
    "    \n",
    "    # Multiclustering regression with confidence interval\n",
    "    slope, intercept, r_value, p_value, std_err = stats.linregress(multi_df['Records_in_Cluster'], multi_df['Rank'])\n",
    "    x_multi = np.linspace(multi_df['Records_in_Cluster'].min(), multi_df['Records_in_Cluster'].max(), 100)\n",
    "    y_multi = slope * x_multi + intercept\n",
    "    \n",
    "    # Plot regression line\n",
    "    plt.plot(x_multi, y_multi, color='#1f77b4', linestyle='-', linewidth=3)\n",
    "    \n",
    "    # Standard regression with confidence interval\n",
    "    slope, intercept, r_value, p_value, std_err = stats.linregress(std_df['Records_in_Cluster'], std_df['Rank'])\n",
    "    x_std = np.linspace(std_df['Records_in_Cluster'].min(), std_df['Records_in_Cluster'].max(), 100)\n",
    "    y_std = slope * x_std + intercept\n",
    "    \n",
    "    # Plot regression line\n",
    "    plt.plot(x_std, y_std, color='#ff7f0e', linestyle='-', linewidth=3)\n",
    "    \n",
    "    # Add shaded confidence interval (requires additional computation)\n",
    "    \n",
    "    # Add annotations\n",
    "    plt.title('Effect of Cluster Size on Ranking: Algorithm Comparison', fontsize=18, fontweight='bold')\n",
    "    plt.xlabel('Number of Records in Cluster', fontsize=14)\n",
    "    plt.ylabel('Rank (Lower is Better)', fontsize=14)\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Add legend\n",
    "    plt.legend(fontsize=12, loc='upper left')\n",
    "    \n",
    "    # Add correlation difference annotation\n",
    "    correlation_diff = std_corr - multi_corr\n",
    "    plt.annotate(f'Correlation Difference: {correlation_diff:.3f}',\n",
    "                xy=(0.02, 0.95), xycoords='axes fraction',\n",
    "                fontsize=12, bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('improved_algorithm_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b5b510d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_binned_analysis_plot(df):\n",
    "    \"\"\"\n",
    "    Create a binned analysis chart showing average rank by record count bins\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): Processed data\n",
    "    \"\"\"\n",
    "    # Create figure\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    # Filter data for each algorithm\n",
    "    multi_df = df[df['Algorithm'] == 'Multiclustering']\n",
    "    std_df = df[df['Algorithm'] == 'Standard']\n",
    "    \n",
    "    # Create bins for records\n",
    "    bins = [0, 2, 4, 6, 8, 10, 12, 14, 16, np.inf]\n",
    "    bin_labels = ['0-2', '3-4', '5-6', '7-8', '9-10', '11-12', '13-14', '15-16', '17+']\n",
    "    \n",
    "    # Function to create binned data\n",
    "    def create_binned_data(algorithm_df):\n",
    "        algorithm_df = algorithm_df.copy()\n",
    "        algorithm_df['RecordBin'] = pd.cut(algorithm_df['Records_in_Cluster'], bins, labels=bin_labels)\n",
    "        binned_data = algorithm_df.groupby('RecordBin').agg({\n",
    "            'Rank': ['mean', 'median', 'count', 'std']\n",
    "        }).reset_index()\n",
    "        binned_data.columns = ['RecordBin', 'MeanRank', 'MedianRank', 'Count', 'StdRank']\n",
    "        return binned_data\n",
    "    \n",
    "    # Create binned data for each algorithm\n",
    "    multi_binned = create_binned_data(multi_df)\n",
    "    std_binned = create_binned_data(std_df)\n",
    "    \n",
    "    # Set up bar positions\n",
    "    bar_width = 0.35\n",
    "    bin_positions = np.arange(len(bin_labels))\n",
    "    \n",
    "    # Plot bars with error bars\n",
    "    plt.bar(bin_positions - bar_width/2, multi_binned['MeanRank'], \n",
    "           width=bar_width, color='#1f77b4', alpha=0.7, label='Multiclustering')\n",
    "    plt.errorbar(bin_positions - bar_width/2, multi_binned['MeanRank'], \n",
    "                yerr=multi_binned['StdRank'] / np.sqrt(multi_binned['Count']), \n",
    "                fmt='none', color='black', capsize=5)\n",
    "    \n",
    "    plt.bar(bin_positions + bar_width/2, std_binned['MeanRank'], \n",
    "           width=bar_width, color='#ff7f0e', alpha=0.7, label='Standard')\n",
    "    plt.errorbar(bin_positions + bar_width/2, std_binned['MeanRank'], \n",
    "                yerr=std_binned['StdRank'] / np.sqrt(std_binned['Count']), \n",
    "                fmt='none', color='black', capsize=5)\n",
    "    \n",
    "    # Add count annotations\n",
    "    for i, count in enumerate(multi_binned['Count']):\n",
    "        plt.text(i - bar_width/2, multi_binned['MeanRank'].iloc[i] + 0.5, \n",
    "                f'n={count}', ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    for i, count in enumerate(std_binned['Count']):\n",
    "        plt.text(i + bar_width/2, std_binned['MeanRank'].iloc[i] + 0.5, \n",
    "                f'n={count}', ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    # Add trend lines\n",
    "    multi_x = bin_positions\n",
    "    multi_y = multi_binned['MeanRank']\n",
    "    std_x = bin_positions\n",
    "    std_y = std_binned['MeanRank']\n",
    "    \n",
    "    # Calculate trend lines\n",
    "    multi_z = np.polyfit(multi_x, multi_y, 1)\n",
    "    multi_p = np.poly1d(multi_z)\n",
    "    \n",
    "    std_z = np.polyfit(std_x, std_y, 1)\n",
    "    std_p = np.poly1d(std_z)\n",
    "    \n",
    "    # Add trend lines\n",
    "    plt.plot(bin_positions, multi_p(bin_positions), 'b--', linewidth=2)\n",
    "    plt.plot(bin_positions, std_p(bin_positions), 'r--', linewidth=2)\n",
    "    \n",
    "    # Add trend line equations\n",
    "    multi_slope, multi_intercept = multi_z\n",
    "    std_slope, std_intercept = std_z\n",
    "    \n",
    "    plt.text(0.05, 0.05, \n",
    "            f\"Multiclustering trend: y = {multi_slope:.3f}x + {multi_intercept:.3f}\\n\"\n",
    "            f\"Standard trend: y = {std_slope:.3f}x + {std_intercept:.3f}\\n\"\n",
    "            f\"Slope difference: {std_slope - multi_slope:.3f}\",\n",
    "            transform=plt.gca().transAxes, fontsize=12, verticalalignment='bottom',\n",
    "            bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    \n",
    "    # Setup plot labels and styling\n",
    "    plt.xlabel('Number of Records in Cluster', fontsize=14)\n",
    "    plt.ylabel('Average Rank (Lower is Better)', fontsize=14)\n",
    "    plt.title('Average Rank by Records in Cluster', fontsize=18, fontweight='bold')\n",
    "    plt.xticks(bin_positions, bin_labels, rotation=45)\n",
    "    plt.grid(True, linestyle='--', alpha=0.3)\n",
    "    plt.legend(fontsize=12)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('binned_analysis_plot.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "764baffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_company_heatmap(df):\n",
    "    \"\"\"\n",
    "    Create a heatmap showing the relationship between records and rank across companies\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): Processed data\n",
    "    \"\"\"\n",
    "    # Create two separate plots for algorithms\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))\n",
    "    \n",
    "    # Filter data for each algorithm\n",
    "    multi_df = df[df['Algorithm'] == 'Multiclustering']\n",
    "    std_df = df[df['Algorithm'] == 'Standard']\n",
    "    \n",
    "    # Prepare data for heatmap - first create company vs records pivot tables\n",
    "    multi_pivot = multi_df.pivot_table(\n",
    "        index='Company', \n",
    "        values='Rank',\n",
    "        columns=pd.cut(multi_df['Records_in_Cluster'], \n",
    "                      bins=[0, 3, 6, 9, 12, 15, 18], \n",
    "                      labels=['0-3', '4-6', '7-9', '10-12', '13-15', '16-18']),\n",
    "        aggfunc='mean'\n",
    "    )\n",
    "    \n",
    "    std_pivot = std_df.pivot_table(\n",
    "        index='Company', \n",
    "        values='Rank',\n",
    "        columns=pd.cut(std_df['Records_in_Cluster'], \n",
    "                      bins=[0, 3, 6, 9, 12, 15, 18], \n",
    "                      labels=['0-3', '4-6', '7-9', '10-12', '13-15', '16-18']),\n",
    "        aggfunc='mean'\n",
    "    )\n",
    "    \n",
    "    # Create custom colormap - lower numbers (better ranks) should be green\n",
    "    colors = ['#1a9850', '#91cf60', '#d9ef8b', '#ffffbf', '#fee08b', '#fc8d59', '#d73027']\n",
    "    cmap = LinearSegmentedColormap.from_list('GreenToRed', colors, N=100)\n",
    "    \n",
    "    # Plot heatmaps\n",
    "    sns.heatmap(multi_pivot, ax=ax1, cmap=cmap, annot=True, fmt='.1f', \n",
    "               linewidths=0.5, vmin=1, vmax=16, cbar_kws={'label': 'Average Rank'})\n",
    "    sns.heatmap(std_pivot, ax=ax2, cmap=cmap, annot=True, fmt='.1f', \n",
    "               linewidths=0.5, vmin=1, vmax=16, cbar_kws={'label': 'Average Rank'})\n",
    "    \n",
    "    # Titles and labels\n",
    "    ax1.set_title('Multiclustering: Average Rank by Company and Records', fontsize=16)\n",
    "    ax2.set_title('Standard: Average Rank by Company and Records', fontsize=16)\n",
    "    \n",
    "    for ax in [ax1, ax2]:\n",
    "        ax.set_ylabel('Company', fontsize=14)\n",
    "        ax.set_xlabel('Records in Cluster', fontsize=14)\n",
    "    \n",
    "    plt.suptitle('Heat Map: Company Performance by Cluster Size', fontsize=20, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.savefig('company_heatmap.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ee38503c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def improved_analysis(df):\n",
    "    \"\"\"\n",
    "    Perform an improved analysis with better visualizations\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): Processed data\n",
    "    \"\"\"\n",
    "    # Create all the improved visualizations\n",
    "    improved_scatter_comparison(df)\n",
    "    create_binned_analysis_plot(df)  \n",
    "    create_company_heatmap(df)\n",
    "    create_significance_plot(df)\n",
    "    \n",
    "    # Additional visualizations you can consider:\n",
    "    # 1. Create a faceted plot by company\n",
    "    # 2. Create a version that analyzes the effect by dataset variation or distance function\n",
    "    # 3. Create an interactive version of these plots with plotly\n",
    "    \n",
    "    print(\"Improved analysis complete. All visualizations saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8392f7d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting recommendation algorithm analysis...\n",
      "Processing Excel file: final_measurements/unbiasing/for_company_size/with_gender_and_age_list_frequency_unbiasing_per_cluster.xlsx\n",
      "Successfully read Excel with 508 rows\n",
      "Successfully created DataFrame with 3487 data points\n",
      "Successfully processed data with 3487 data points\n",
      "Generating records vs rank visualization...\n",
      "Records vs rank visualizations saved\n",
      "Generating rank distribution visualization...\n",
      "Rank distribution visualization saved\n",
      "Generating normalized rank visualization...\n",
      "Normalized rank visualization saved\n",
      "Generating correlation comparison by company...\n",
      "Correlation comparison by company saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adler\\AppData\\Local\\Temp\\ipykernel_26884\\1278906343.py:23: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  binned_data = algorithm_df.groupby('RecordBin').agg({\n",
      "C:\\Users\\adler\\AppData\\Local\\Temp\\ipykernel_26884\\1278906343.py:23: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  binned_data = algorithm_df.groupby('RecordBin').agg({\n",
      "C:\\Users\\adler\\AppData\\Local\\Temp\\ipykernel_26884\\4063551401.py:16: FutureWarning: The default value of observed=False is deprecated and will change to observed=True in a future version of pandas. Specify observed=False to silence this warning and retain the current behavior\n",
      "  multi_pivot = multi_df.pivot_table(\n",
      "C:\\Users\\adler\\AppData\\Local\\Temp\\ipykernel_26884\\4063551401.py:25: FutureWarning: The default value of observed=False is deprecated and will change to observed=True in a future version of pandas. Specify observed=False to silence this warning and retain the current behavior\n",
      "  std_pivot = std_df.pivot_table(\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'create_significance_plot' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 40\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAnalysis complete. All visualizations and reports have been saved.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m: \n\u001b[1;32m---> 40\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[29], line 29\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     26\u001b[0m compare_correlations_by_company(df)\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Add improved visualizations\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m \u001b[43mimproved_analysis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Calculate statistics\u001b[39;00m\n\u001b[0;32m     32\u001b[0m stats \u001b[38;5;241m=\u001b[39m calculate_statistics(df)\n",
      "Cell \u001b[1;32mIn[26], line 12\u001b[0m, in \u001b[0;36mimproved_analysis\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m     10\u001b[0m create_binned_analysis_plot(df)  \n\u001b[0;32m     11\u001b[0m create_company_heatmap(df)\n\u001b[1;32m---> 12\u001b[0m \u001b[43mcreate_significance_plot\u001b[49m(df)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Additional visualizations you can consider:\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# 1. Create a faceted plot by company\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# 2. Create a version that analyzes the effect by dataset variation or distance function\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# 3. Create an interactive version of these plots with plotly\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImproved analysis complete. All visualizations saved.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'create_significance_plot' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1400x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to run the analysis\n",
    "    \"\"\"\n",
    "    print(\"Starting recommendation algorithm analysis...\")\n",
    "    \n",
    "    # Process the data file - update this to your actual file path\n",
    "    file_path = \"final_measurements/unbiasing/for_company_size/with_gender_and_age_list_frequency_unbiasing_per_cluster.xlsx\"\n",
    "    \n",
    "    # Process the data\n",
    "    df = process_excel_data(file_path)\n",
    "    \n",
    "    if len(df) == 0:\n",
    "        print(\"No valid data was processed. Please check your data file format.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Successfully processed data with {len(df)} data points\")\n",
    "    \n",
    "    # Save the processed data to CSV for further analysis if needed\n",
    "    df.to_csv('processed_recommendation_data.csv', index=False)\n",
    "    \n",
    "    # Run original visualizations\n",
    "    visualize_records_vs_rank(df)\n",
    "    visualize_rank_distribution(df)\n",
    "    create_normalized_rank_plot(df)\n",
    "    compare_correlations_by_company(df)\n",
    "    \n",
    "    # Add improved visualizations\n",
    "    improved_analysis(df)\n",
    "    \n",
    "    # Calculate statistics\n",
    "    stats = calculate_statistics(df)\n",
    "    \n",
    "    # Generate final report\n",
    "    create_analysis_report(df, stats)\n",
    "    \n",
    "    print(\"\\nAnalysis complete. All visualizations and reports have been saved.\")\n",
    "    \n",
    "if __name__ == \"__main__\": \n",
    "    main()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
